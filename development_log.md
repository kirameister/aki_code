# 開発ログ


## 2023-08-19

まずは常用漢字を、学ぶ小学校と中学校の学年ごとに用意する。`scripts/kanji.json` がその結果できたものである。

Wikipedia の日本語記事データから常用漢字に限定した uni-gram (つまり頻度だけ) を取り出し頻度でソートしてみる。ソートするために、最初は dict() で保存していたのに list() 形式で JSON には出力することになった。

```
python ./scripts/populate_kanji_occurrence_from_wikipedia.py
```

(ちなみに公開している JSON ファイル ([wikipedia_joyo_kanji_freq.json](https://github.com/kirameister/aki_code/blob/development/data/wikipedia_joyo_kanji_freq.json)) はその後手動で整形したものである)

その頻度データと、学ぶ漢字の学年 (中学校はまとめて 7 としてカウント) を並べてみる。Linear regression にしてみる。

```
python ./scripts/linear_regression_betweeen_occur_and_year.py
r^2 score: 0.45773262009027904
```

そしてプロットした図がこちらである。

![Wikipeda Kanji occurrence rank vs. educational year](./image/wikipedia_kanji_occur_rank_to_educational_year.png "Wikipedia の頻出漢字ランキングと学習年のプロット")

率直に言うと、「思ったほど correlate してないな」と感じた。もう少し綺麗にまとまっていれば (そこまで分散していなければ) 遠慮なく小学校の低学年で学ぶ漢字を優先して配列に追加できるのにな、ということなのだが、そう簡単な話でもない、ということなのだろう。もう少しどうするか考える。

## 2023-08-20

Wikipedia からのデータも bi-gram 以上の n-gram として抽出しておく。例によって list() としてソートしておく (公開しているファイルは整形済み…と思ったらそのままで 200MB 以上のデータになっていたので (データ件数だけだと 10M いかないぐらい)、1M で cut-off しておく - それでも多すぎるという可能性は大いにあるけれど - [n_gram_to_weight.json](https://github.com/kirameister/aki_code/blob/development/data/n_gram_to_weight.json))。

```
python ./scripts/n_gram_generator.py
head ./data/n_gram_to_weight.json
[
    [
        "日本",
        594779
    ],
    [
        "現在",
        246779
    ],
    [
```

…データを眺めていると、やはりというか、最初は bi-gram ばかり出てくる。しかしこれ、実装方法がなにか間違っている気がしてきた (主に 4-gram 以上のトークンの扱いについて)。それに人間の脳にはそこまで容量があるわけでもないので、高々 (文脈を知るための) tri-gram で十分なのでは、という気がしてきた。後日 script を変更して試してみよう。

…さっさと変更してしまえ、ということで変更してみた。今度は cut-off も 10k という大胆さ(?)。またにらめっこすることにする。

## 2023-08-21

大岡俊彦さんのブログでいくつかやり取りが発生する。

* http://oookaworks.seesaa.net/article/500397294.html
* http://oookaworks.seesaa.net/article/500430534.html
* http://oookaworks.seesaa.net/article/500430829.html
* http://oookaworks.seesaa.net/article/500430949.html
* http://oookaworks.seesaa.net/article/500433316.html

AKI-code は最初から目指している方向が違うので、全てのポイントがそのまま当てはまるわけではないのだが、やっぱり覚えられない/覚えにくいというのはクリティカルと言うのは間違いなさそう (覚えていない=存在しないストローク)。

そういう観点から熟語ベースで配列を考えていく、という考えに至った。極論すると、あまり使われないような漢字でも、覚えやすければ採用する方が良いだろうという感じ (勿論限度はあるが…)。運指については「記憶のしやすさと比較すると重要度はだいぶ落ちる」という認識。そのことを考えながら、もう少し具体的に掘り下げていくと…

1. 数字あたりを一つの出発点とする (これ自体は覚えるのが簡単)
2. 「左右」や「上下」といった比較的わかりやすい反義語 (特に方向が絡むもの) も一つの出発点として使える

とか考えた。

数字についてだが、/11/ → '一' といった配列を今考えている。これは以下のような理由から:

* 一、二、三を除くと top 10% 以下の出現頻度ランキング
* しかし数値が含まれる熟語はそこそこある (かも? 要確認)

```
$ cat -n ./wikipedia_joyo_kanji_freq.json | grep -E '[一二三四五六七八九零〇十百千万億兆]'
    13	{"一": [1328052,"1"]},
   147	{"三": [374838,"1"]},
   197	{"二": [305472,"1"]},
   314	{"万": [206767,"2"]},
   465	{"十": [134809,"1"]},
   522	{"四": [115231,"1"]},
   556	{"千": [103823,"1"]},
   595	{"五": [94833,"1"]},
   627	{"八": [88326,"1"]},
   748	{"九": [68232,"1"]},
   863	{"百": [52344,"1"]},
   869	{"六": [51528,"1"]},
   924	{"億": [45416,"4"]},
   964	{"七": [41194,"1"]},
  1659	{"兆": [8693,"4"]},
  1759	{"零": [6542,"7"]},
$ cat -n ./n_gram_to_weight.json | grep -E '[一二三四五六七八九零〇十百千万億兆]' | head
    22	["一部",124532],
    33	["一般",108735],
    42	["一方",100930],
    87	["第二",65670],
   103	["第一",59553],
   188	["一人",44697],
   224	["第二次",40669],
   250	["一般的",38367],
   277	["一時",35723],
   341	["二次世",30646],
```

次に「反義語」だが、今の段階では

* /a;/ → 右
* /;a/ → 左

というのを考えている。

ところで、これらの配列表 (ストローク表?) ってどういう representation が良いだろうね? 機械が読むことを前提に JSON で記述するというのは安直だけど、それだと全体を把握するのは面倒かも。Validation のようなものを用意すれば良い話かも?


## 2023-08-24

どのようにして計算機を使って配列の構築 (開発、と言っても良いのか?) するか考えてみた。全てを完全自動化することはできない/したくないと考えているのだが、こういう方法ならあり得るかも知れないと思った。

* 何らかの形で熟語と漢字にスコアを与える (normalize)
* すでに配列上にある漢字に、(熟語として) 組み合わせることのできる漢字をランク付け
    * 例: `候補の漢字スコア = alpha * sum(熟語スコア) + その漢字のスコア` (ただし熟語スコアは候補の漢字が現れるときに限る)

日本語の式だとわかりにくいね。コード書いてみるか。

…というわけで書いてみた。`_normalized.json` というファイルがそれである。`log10()` を使っているが、わざわざ log の底を 10 にしたのに大きな理由はない。強いて言えば、個々の漢字の出現数よりも、様々な熟語でその漢字が使われている方がスコアをより高くしたかったからだろう (それならばもう少し cut-off を控えめにしたほうが良いかも。これはまた調整するかも)。

あれからもう少し考えて、以下の漢字を (新たに) 追加しようと考えた (簡単に覚えられるだろうというのがその理由):

* 数字の一から九 (既出)
* 右, 左 (既出)
* 東西南北
* 上下

これらの内容を `data/stroke_layout.json` に記述してみた。これをベースに関連する熟語をランク付けするのだが、`alpha=0.5` の場合、こういう感じになる:

```
python ./scripts/suggest_next_kanji_candidate_dfs.py | head -n 5
('大', 31.685197700258666)
('部', 27.229915332465453)
('第', 23.37900931251443)
('方', 23.340216787662317)
('年', 20.277657810429055)
```

ふーむ。これだけ見てもよく分からない。どういう (既に定義された) 漢字から導出されたのか知りたいので、今度はその辺を出力するスクリプトを書いてみよう (ついでに alpha の値も argparse で読めるようにしておけ、という話もある)。

