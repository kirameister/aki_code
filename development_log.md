# 開発ログ


## 2023-08-19

まずは常用漢字を、学ぶ小学校と中学校の学年ごとに用意する。`scripts/kanji.json` がその結果できたものである。

Wikipedia の日本語記事データから常用漢字に限定した uni-gram (つまり頻度だけ) を取り出し頻度でソートしてみる。ソートするために、最初は dict() で保存していたのに list() 形式で JSON には出力することになった。

```
python ./scripts/populate_kanji_occurrence_from_wikipedia.py
```

(ちなみに公開している JSON ファイル ([wikipedia_joyo_kanji_freq.json](https://github.com/kirameister/aki_code/blob/development/data/wikipedia_joyo_kanji_freq.json)) はその後手動で整形したものである)

その頻度データと、学ぶ漢字の学年 (中学校はまとめて 7 としてカウント) を並べてみる。Linear regression にしてみる。

```
python ./scripts/linear_regression_betweeen_occur_and_year.py
r^2 score: 0.45773262009027904
```

そしてプロットした図がこちらである。

![Wikipeda Kanji occurrence rank vs. educational year](./image/wikipedia_kanji_occur_rank_to_educational_year.png "Wikipedia の頻出漢字ランキングと学習年のプロット")

率直に言うと、「思ったほど correlate してないな」と感じた。もう少し綺麗にまとまっていれば (そこまで分散していなければ) 遠慮なく小学校の低学年で学ぶ漢字を優先して配列に追加できるのにな、ということなのだが、そう簡単な話でもない、ということなのだろう。もう少しどうするか考える。

## 2023-08-20

Wikipedia からのデータも bi-gram 以上の n-gram として抽出しておく。例によって list() としてソートしておく (公開しているファイルは整形済み…と思ったらそのままで 200MB 以上のデータになっていたので (データ件数だけだと 10M いかないぐらい)、1M で cut-off しておく - それでも多すぎるという可能性は大いにあるけれど - [n_gram_to_weight.json](https://github.com/kirameister/aki_code/blob/development/data/n_gram_to_weight.json))。

```
python ./scripts/n_gram_generator.py
head ./data/n_gram_to_weight.json
[
    [
        "日本",
        594779
    ],
    [
        "現在",
        246779
    ],
    [
```

…データを眺めていると、やはりというか、最初は bi-gram ばかり出てくる。しかしこれ、実装方法がなにか間違っている気がしてきた (主に 4-gram 以上のトークンの扱いについて)。それに人間の脳にはそこまで容量があるわけでもないので、高々 (文脈を知るための) tri-gram で十分なのでは、という気がしてきた。後日 script を変更して試してみよう。

…さっさと変更してしまえ、ということで変更してみた。今度は cut-off も 10k という大胆さ(?)。またにらめっこすることにする。

## 2023-08-21

大岡俊彦さんのブログでいくつかやり取りが発生する。

* http://oookaworks.seesaa.net/article/500397294.html
* http://oookaworks.seesaa.net/article/500430534.html
* http://oookaworks.seesaa.net/article/500430829.html
* http://oookaworks.seesaa.net/article/500430949.html
* http://oookaworks.seesaa.net/article/500433316.html

AKI-code は最初から目指している方向が違うので、全てのポイントがそのまま当てはまるわけではないのだが、やっぱり覚えられない/覚えにくいというのはクリティカルと言うのは間違いなさそう (覚えていない=存在しないストローク)。

そういう観点から熟語ベースで配列を考えていく、という考えに至った。極論すると、あまり使われないような漢字でも、覚えやすければ採用する方が良いだろうという感じ (勿論限度はあるが…)。運指については「記憶のしやすさと比較すると重要度はだいぶ落ちる」という認識。そのことを考えながら、もう少し具体的に掘り下げていくと…

1. 数字あたりを一つの出発点とする (これ自体は覚えるのが簡単)
2. 「左右」や「上下」といった比較的わかりやすい反義語 (特に方向が絡むもの) も一つの出発点として使える

とか考えた。

数字についてだが、/11/ → '一' といった配列を今考えている。これは以下のような理由から:

* 一、二、三を除くと top 10% 以下の出現頻度ランキング
* しかし数値が含まれる熟語はそこそこある (かも? 要確認)

```
$ cat -n ./wikipedia_joyo_kanji_freq.json | grep -E '[一二三四五六七八九零〇十百千万億兆]'
    13	{"一": [1328052,"1"]},
   147	{"三": [374838,"1"]},
   197	{"二": [305472,"1"]},
   314	{"万": [206767,"2"]},
   465	{"十": [134809,"1"]},
   522	{"四": [115231,"1"]},
   556	{"千": [103823,"1"]},
   595	{"五": [94833,"1"]},
   627	{"八": [88326,"1"]},
   748	{"九": [68232,"1"]},
   863	{"百": [52344,"1"]},
   869	{"六": [51528,"1"]},
   924	{"億": [45416,"4"]},
   964	{"七": [41194,"1"]},
  1659	{"兆": [8693,"4"]},
  1759	{"零": [6542,"7"]},
$ cat -n ./n_gram_to_weight.json | grep -E '[一二三四五六七八九零〇十百千万億兆]' | head
    22	["一部",124532],
    33	["一般",108735],
    42	["一方",100930],
    87	["第二",65670],
   103	["第一",59553],
   188	["一人",44697],
   224	["第二次",40669],
   250	["一般的",38367],
   277	["一時",35723],
   341	["二次世",30646],
```

次に「反義語」だが、今の段階では

* /a;/ → 右
* /;a/ → 左

というのを考えている。

ところで、これらの配列表 (ストローク表?) ってどういう representation が良いだろうね? 機械が読むことを前提に JSON で記述するというのは安直だけど、それだと全体を把握するのは面倒かも。Validation のようなものを用意すれば良い話かも?


## 2023-08-24

どのようにして計算機を使って配列の構築 (開発、と言っても良いのか?) するか考えてみた。全てを完全自動化することはできない/したくないと考えているのだが、こういう方法ならあり得るかも知れないと思った。

* 何らかの形で熟語と漢字にスコアを与える (normalize)
* すでに配列上にある漢字に、(熟語として) 組み合わせることのできる漢字をランク付け
    * 例: `候補の漢字スコア = alpha * sum(熟語スコア) + その漢字のスコア` (ただし熟語スコアは候補の漢字が現れるときに限る)

日本語の式だとわかりにくいね。コード書いてみるか。

…というわけで書いてみた。`_normalized.json` というファイルがそれである。`log10()` を使っているが、わざわざ log の底を 10 にしたのに大きな理由はない。強いて言えば、個々の漢字の出現数よりも、様々な熟語でその漢字が使われている方がスコアをより高くしたかったからだろう (それならばもう少し cut-off を控えめにしたほうが良いかも。これはまた調整するかも)。

あれからもう少し考えて、以下の漢字を (新たに) 追加しようと考えた (簡単に覚えられるだろうというのがその理由):

* 数字の一から九 (既出)
* 右, 左 (既出)
* 東西南北
* 上下

これらの内容を `data/stroke_layout.json` に記述してみた。これをベースに関連する熟語をランク付けするのだが、`alpha=0.5` の場合、こういう感じになる:

```
python ./scripts/suggest_next_kanji_candidate_dfs.py | head -n 5
('大', 31.685197700258666)
('部', 27.229915332465453)
('第', 23.37900931251443)
('方', 23.340216787662317)
('年', 20.277657810429055)
```

ふーむ。これだけ見てもよく分からない。どういう (既に定義された) 漢字から導出されたのか知りたいので、今度はその辺を出力するスクリプトを書いてみよう (ついでに alpha の値も argparse で読めるようにしておけ、という話もある)。

…恥ずかしいことに式が間違っていたということに今更気がつく。後で更新します。


## 2023/08/24

計算式間違っていたという大分恥ずかしいことをしてしまったのだけれど、一旦スクリプトの名前と生成ファイル名を整理してみることにする。

* 数字の prefix をスクリプト名に入れる -- 昔やったことのある簡単ハックなのだけれど、案外役に立つ。
* `freq` を `occurr` に変更 (`occurr` は `occurrence` の略; `occur` だと動詞と混合しそうでやめた) -- これは freq という表現が単純に気持ち悪かったから。

計算式については、一旦個々の wikipedia n-gram の重みを全て忘れて、単純に occurrence の数だけで勝負したらどうなるのか興味を持ってみた。これは「高いスコアが n-gram から得られたから」よりも「覚えやすいストローク」を求めるべきで、そのための候補は多いほうが良いだろうと考えたため。N-gram の生成 cut-off も 10k から 100k にして走らせてみる。

単純に n-gram のスコアを無視して、全ての重みを 1 として出力してみると、なかなか面白い (というかより使えそうな情報が出てきた)。

```
python ./scripts/4.suggest_next_kanji_candidate_dfs.py | head
('十', 56.0)
('大', 33.0)
('部', 28.0)
('方', 25.0)
('国', 24.0)
('年', 23.0)
('中', 22.0)
('郎', 22.0)
('州', 21.0)
('新', 21.0)
..
```

単なる数値では味気ない (というはそこから元データに grep かけるのが面倒な) ので、実際にマッチした n-gram を出力するほうが良いかも知れない。

…というわけで更新してみた:

```
python ./scripts/4.suggest_next_kanji_candidate_dfs.py | head -n 3
('十', ['十二', '十二', '五十', '三十', '二十', '十三', '十六', '十一', '八十', '四十', '九十九', '三十三', '九十', '十八', '十五', '二十四', '三十六', '八十八', '十四', '五十六', '十七', '七十', '二十一', '二十五', '六十', '二十二', '二十八', '東十', '三十一', '西十', '四十八', '二十三', '二十六', '三十五', '四十七', '五十三', '二十七', '三十七', '三十二', '十九', '四十九', '二十九', '三十四', '三十八', '七十七', '四十二', '四十四', '六十四', '南十', '六十六', '四十一', '五十一', '五十二', '五十四', '六十九', '三十九', '八十二'])
('大', ['大西', '大西', '東大', '大東', '一大', '東北大', '北大', '三大', '五大', '二大', '四大', '西大', '南大', '北大西', '右大', '左大', '八大', '上大', '大北', '大下', '六大', '大一', '大八', '大三', '七大', '西六大', '下大', '南大西', '九大', '南大東', '大五', '三大大', '大南', '西大西'])
('部', ['一部', '一部', '南部', '北部', '西部', '東部', '下部', '上部', '部下', '北西部', '北東部', '南西部', '南東部', '三部', '東北部', '二部', '東南部', '西南部', '西北部', '北部九', '四部', '一部上', '五部', '六部', '部左右', '一部部', '部上', '部一', '南部一'])
...
```

…なんだかこれはグラフ問題なのではないかと薄々考えているのだが、既にストローク表にある漢字が tri-gram にある場合、そこをブーストするような仕組みとかあっても良いかも (またはそれ用のオプションをつけるとか)。

…というわけでそういうオプションを付けてみた:

```
python ./scripts/4.suggest_next_kanji_candidate_dfs.py --mesh_mode | head
('十', ['九十九', '九十九', '三十三', '二十四', '三十六', '八十八', '五十六', '二十一', '二十五', '二十二', '二十八', '三十一', '四十八', '二十三', '二十六', '三十五', '四十七', '五十三', '二十七', '三十七', '三十二', '四十九', '二十九', '三十四', '三十八', '七十七', '四十二', '四十四', '六十四', '六十六', '四十一', '五十一', '五十二', '五十四', '六十九', '三十九', '八十二'])
('部', ['北西部', '北西部', '北東部', '南西部', '南東部', '東北部', '東南部', '西南部', '西北部', '北部九', '一部上', '部左右', '一部部', '南部一'])
('郎', ['三四郎', '三四郎', '八五郎', '四郎左', '三郎左', '八郎左', '三五郎', '五郎左', '五郎右', '郎三郎', '三郎右', '東一郎', '八郎右', '九郎右'])
('州', ['北九州', '北九州', '南九州', '九州北', '州南西', '州南東', '九州南', '州北西', '州北東', '東九州', '西九州', '九州西'])
('方', ['南西方', '南西方', '南東方', '東西方', '南北方', '北西方', '北東方', '左右方', '東北方', '上下方', '上四方', '東南方'])
('京', ['東京六', '東京六', '西東京', '北京五', '東京五', '東京三', '東京一', '一東京', '二東京', '東京上', '東東京', '東京下'])
('式', ['九七式', '九七式', '九九式', '九六式', '九四式', '八九式', '九五式', '九二式', '三八式', '九三式', '九八式', '九一式'])
('国', ['国東北', '国東北', '四国八', '一国一', '国北東', '西国三', '国南西', '国三国', '国南東', '国四国', '一国二', '国北西'])
('位', ['五位下', '五位下', '四位下', '五位上', '四位上', '三位一', '六位上', '下上位', '六位下'])
('衛', ['五右衛', '五右衛', '三右衛', '八右衛', '三左衛', '五左衛', '六右衛', '六左衛', '七左衛'])
```

なるほど、なかなか面白い。'郎' はおそらく名前から多く取り出されていそうだし、'州' は '九州' から得られたものが殆どという印象を受ける。

## 2023-08-25

以下の記事を読んだ:

* http://oookaworks.seesaa.net/article/500496692.html

同音異義語というのは確かに大きなテーマ、というか AKI-code 始めようと思った主な理由なのだけれど、改めて調べてみると結構ある:

* https://yattoke.com/2018/05/11/homonym/
* https://dank.jp/blog/homonym/

あるべき姿としては、よくある同音異義語や confusing な単語/言い回しに含まれる漢字を取り出して、ストロークの中に入れるということになるのだろうけれど、中には「遺骸」とか「歓心」といった普段使わないような表現もある (少なくとも日常生活で「遺骸」なんて言葉書いた記憶ないぞ)。この辺は結構底なし沼というか、多分どんなに頑張っても (最大でも 40 x 40 = 1600 という制約で) 抜け落ちる漢字は出てくるのだろうなと想像する。

…そこで考えたのが、AKI-code としてはいくつかの「レベル分け」をするというもの。

1. core: `/1/` から `/0/` までのダブルストローク + `/[uiojklm,.]{,2}/` + `/[wersdfxcv]{,2}/` = 10 + 9^2 * 2 = 172
2. core-ext: `/1/` から `/0/` までのダブルストローク + `/[uiojklm,.wersdfxcv]{,2}/` = 10 + 18^2 = 334
3. (名前未定 -- ext??): それ以上

最低限:
* 覚えやすい
* 同音異義語に出てくる漢字をできるだけカバーする

ような漢字を優先的に core から配置して、ある意味それ以外はユーザー任せにしてしまうとか? しかしそれはそれで無責任な気も大いにする。とりあえず SKK dictionary の同音異義語を眺めてみようか。

https://github.com/skk-dev/dict/blob/master/SKK-JISYO.S





